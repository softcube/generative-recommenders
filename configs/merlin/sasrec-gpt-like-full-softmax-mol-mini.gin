# SASRec config approximating your small GPT-style transformer
# on Merlin session data in tmp/merlin, using full softmax loss
# and MoL (Mixture-of-Logits) similarity instead of plain dot product.
#
# This lets you compare:
#   - SASRec + DotProduct (sasrec-gpt-like-full-softmax-mini.gin)
#   - vs SASRec + MoL similarity at similar scale.
#
# Preprocessing (no prefix expansion; expand_sequences_to_prefixes=False):
#   python3 preprocess_merlin_data.py
#
# Run training as:
#   python3 main.py \
#     --gin_config_file=configs/merlin/sasrec-gpt-like-full-softmax-mol-mini.gin \
#     --master_port=12345

train_fn.dataset_name = "merlin"

# Approximate GPT block size 20: use 20 historical steps.
train_fn.max_sequence_length = 20

# No extra generative outputs beyond next-item prediction.
train_fn.gr_output_length = 0

# Keep batch size modest for CPU/MPS (can be reduced if needed).
train_fn.local_batch_size = 2048
train_fn.eval_batch_size = 128

# Use SASRec (Transformer) as the main module to mimic GPT.
train_fn.main_module = "SASRec"

# Match low dropout from your GPT (0.01).
train_fn.dropout_rate = 0.01
train_fn.user_embedding_norm = "l2_norm"

# Use rated input preprocessor: concatenate item embeddings
# with event-type embeddings derived from ratings.
train_fn.use_rated_input_preproc = True
# Match your old event embedding dim (8) and vocab size (6).
train_fn.rating_embedding_dim = 8
train_fn.num_ratings = 6

# Single-epoch run for quick experiments.
train_fn.num_epochs = 10

# Match GPT-style capacity with smaller trainable embeddings.
train_fn.item_embedding_dim = 256

# Transformer depth and heads.
sasrec_encoder.num_blocks = 4
sasrec_encoder.num_heads = 2

# FFN hidden dim ~ 3x model dim (256 -> 768).
sasrec_encoder.ffn_hidden_dim = 768
sasrec_encoder.ffn_activation_fn = "gelu"

# Use small dropout in attention/FFN to match GPT (0.01).
sasrec_encoder.ffn_dropout_rate = 0.01

train_fn.learning_rate = 1e-3
train_fn.weight_decay = 0
train_fn.num_warmup_steps = 0

# MoL similarity + MoL-aware brute-force Top-K.
train_fn.interaction_module_type = "MoL"
train_fn.top_k_method = "MoLBruteForceTopK"

# Use full softmax over all items (closest to GPT LM CE).
train_fn.loss_module = "FullSoftmaxLoss"
train_fn.temperature = 1.0
train_fn.main_module_bf16 = True
get_similarity_function.bf16_training = True

# MoL hyperparameters (see create_mol_interaction_module in similarity_utils.py).
# These control the number/size of component logits and gating MLPs.
create_mol_interaction_module.dot_product_dimension = 128
create_mol_interaction_module.query_dot_product_groups = 4
create_mol_interaction_module.item_dot_product_groups = 4
create_mol_interaction_module.temperature = 1.0
create_mol_interaction_module.query_dropout_rate = 0.01

create_mol_interaction_module.query_hidden_dim = 1024
create_mol_interaction_module.item_dropout_rate = 0.01
create_mol_interaction_module.item_hidden_dim = 1024
create_mol_interaction_module.gating_query_hidden_dim = 512
create_mol_interaction_module.gating_qi_hidden_dim = 512
create_mol_interaction_module.gating_item_hidden_dim = 512
create_mol_interaction_module.softmax_dropout_rate = 0.0

# Sampling strategy is ignored by FullSoftmaxLoss but kept for completeness.
train_fn.sampling_strategy = "local"
train_fn.item_l2_norm = True
train_fn.l2_norm_eps = 1e-6

# Eval only after each epoch; keep eval cheap.
train_fn.eval_interval = 1000000000  # effectively disable mid-epoch eval
train_fn.full_eval_every_n = 1       # eval after each epoch
train_fn.partial_eval_num_iters = 1
train_fn.save_ckpt_every_n = 1       # save every epoch
train_fn.throughput_log_interval = 100

train_fn.enable_tf32 = True

# Single-process, no worker subprocesses for macOS.
create_data_loader.prefetch_factor = 4
create_data_loader.num_workers = 8
